# Mixture Models

## Overview

Data is often **heterogeneous**, containing multiple subgroups or sub-populations of a larger data set. **Mixture Models** are comprised of linear components of simple distributions

General form of a mixture distribution: 

$$
f(\chi) = \sum_{k=1}^{K}\pi_kf_k(\chi; \omega_k)
$$

where there are K components (each is k)

- $f_k(\chi;\omega)$ is the distribution for component k
- 􏰀$\omega_k$ are the parameters for component k
- $\pi_k$ is the weighting of component k in the mixture, $0 \leq \pi_k \leq 1$, i.e., the probability that a randomly chosen data point was generated by component k
- all $\pi_k$’s sum to 1


## Mixture models

### Gaussian Mixture Models

A **Gaussian Mixture Model (GMM)** is a probabilistic version of K-Means clustering.

A GMM can be represented as:

$$
Pr(\chi_j) = \sum_{k=1}^{K}Pr(C_k)Pr(\chi_j|C_k)
$$

- $\chi_j$ represents one observation (instance),
- $C_k$ represents the component of the mixture model, where there are k components.

## Instance-based Models

### Memory Based Reasoning

### Nearest Neighbours

### Collaborative Filtering

### Compuational Aspects


